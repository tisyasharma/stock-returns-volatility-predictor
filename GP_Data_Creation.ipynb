{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6667c189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ta\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# Dynamic timeline (end is exclusive, so add a day to grab latest close)\n",
    "start_date = date(2015, 1, 1)\n",
    "end_date = date.today() + timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 tickers\n"
     ]
    }
   ],
   "source": [
    "# Cross-sector mix for future modeling\n",
    "tickers = [\"AAPL\",\"MSFT\",\"AMZN\",\"GOOGL\",\"NVDA\",\"TSLA\",\"JPM\",\"WMT\",\n",
    "           \"DAL\",\"UAL\",\"LMT\",\"RTX\",\"NOC\",\"XOM\"]\n",
    "\n",
    "print(f\"{len(tickers)} tickers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  14 of 14 completed\n"
     ]
    }
   ],
   "source": [
    "# Download price data\n",
    "data = yf.download(tickers, start=start_date, end=end_date, auto_adjust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Price</th>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>24.237553</td>\n",
       "      <td>24.705322</td>\n",
       "      <td>23.798602</td>\n",
       "      <td>24.694237</td>\n",
       "      <td>212818400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>15.426000</td>\n",
       "      <td>15.737500</td>\n",
       "      <td>15.348000</td>\n",
       "      <td>15.629000</td>\n",
       "      <td>55664000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>DAL</td>\n",
       "      <td>43.064999</td>\n",
       "      <td>43.791795</td>\n",
       "      <td>42.653437</td>\n",
       "      <td>43.712986</td>\n",
       "      <td>8637300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>26.278944</td>\n",
       "      <td>26.589101</td>\n",
       "      <td>26.196068</td>\n",
       "      <td>26.430299</td>\n",
       "      <td>26480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>JPM</td>\n",
       "      <td>46.720932</td>\n",
       "      <td>47.072328</td>\n",
       "      <td>46.406916</td>\n",
       "      <td>46.489158</td>\n",
       "      <td>12600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price       Date Ticker      Close       High        Low       Open     Volume\n",
       "0     2015-01-02   AAPL  24.237553  24.705322  23.798602  24.694237  212818400\n",
       "1     2015-01-02   AMZN  15.426000  15.737500  15.348000  15.629000   55664000\n",
       "2     2015-01-02    DAL  43.064999  43.791795  42.653437  43.712986    8637300\n",
       "3     2015-01-02  GOOGL  26.278944  26.589101  26.196068  26.430299   26480000\n",
       "4     2015-01-02    JPM  46.720932  47.072328  46.406916  46.489158   12600000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten multi-index into one row per ticker and date\n",
    "data_flat = data.stack(level=1, future_stack=True).reset_index()\n",
    "\n",
    "data_flat.rename(columns={\n",
    "    \"level_1\": \"Ticker\",\n",
    "    \"Adj Close\": \"AdjClose\",\n",
    "    \"Close\": \"Close\",\n",
    "    \"Open\": \"Open\",\n",
    "    \"High\": \"High\",\n",
    "    \"Low\": \"Low\",\n",
    "    \"Volume\": \"Volume\"\n",
    "}, inplace=True)\n",
    "\n",
    "data_flat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: technical indicators\n",
    "data_flat[\"Return\"] = data_flat.groupby(\"Ticker\")[\"Close\"].pct_change()\n",
    "\n",
    "data_flat[\"RollingVol\"] = (\n",
    "    data_flat.groupby(\"Ticker\")[\"Return\"]\n",
    "    .rolling(window=10)\n",
    "    .std()\n",
    "    .reset_index(0, drop=True)\n",
    ")\n",
    "\n",
    "data_flat[\"RSI\"] = data_flat.groupby(\"Ticker\")[\"Close\"].transform(\n",
    "    lambda x: ta.momentum.rsi(x, window=14)\n",
    ")\n",
    "\n",
    "# SMA ratios instead of absolute levels to avoid price scale leakage\n",
    "sma_20 = data_flat.groupby(\"Ticker\")[\"Close\"].transform(\n",
    "    lambda x: x.rolling(20).mean()\n",
    ")\n",
    "sma_50 = data_flat.groupby(\"Ticker\")[\"Close\"].transform(\n",
    "    lambda x: x.rolling(50).mean()\n",
    ")\n",
    "\n",
    "data_flat[\"Price_to_SMA20\"] = data_flat[\"Close\"] / sma_20\n",
    "data_flat[\"Price_to_SMA50\"] = data_flat[\"Close\"] / sma_50\n",
    "data_flat[\"SMA20_to_SMA50\"] = sma_20 / sma_50\n",
    "\n",
    "data_flat[\"Volume_Z\"] = data_flat.groupby(\"Ticker\")[\"Volume\"].transform(\n",
    "    lambda x: (x - x.rolling(20).mean()) / x.rolling(20).std()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leading VIX NaNs: 0\n"
     ]
    }
   ],
   "source": [
    "# Merge VIX index (leakage-safe forward-fill)\n",
    "macro = yf.download(\n",
    "    [\"^VIX\"],\n",
    "    start=start_date,\n",
    "    end=end_date,\n",
    "    auto_adjust=True\n",
    ")\n",
    "\n",
    "macro.columns = [col[0] for col in macro.columns]\n",
    "macro = macro.reset_index().rename(columns={\"Close\": \"VIX\"})\n",
    "\n",
    "# Forward-fill VIX BEFORE merge to avoid cross-ticker leakage\n",
    "macro = macro.sort_values(\"Date\")\n",
    "macro[\"VIX\"] = macro[\"VIX\"].ffill()\n",
    "\n",
    "# Merge VIX by date\n",
    "data_flat = data_flat.merge(macro[[\"Date\", \"VIX\"]], on=\"Date\", how=\"left\")\n",
    "\n",
    "# Sanity check: leading NaNs (before VIX data starts)\n",
    "print(f\"Leading VIX NaNs: {data_flat['VIX'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Sort by Ticker and Date to ensure proper time ordering\ndata_flat = data_flat.sort_values([\"Ticker\", \"Date\"]).reset_index(drop=True)\n\n# Lagged_Return: prior day's return (t-1) for predicting t+1 outcomes\ndata_flat[\"Lagged_Return\"] = data_flat.groupby(\"Ticker\")[\"Return\"].shift(1)\n\n\n# Create targets (t â†’ t+1 labels, leakage-safe)\n\n# NextReturn: tomorrow's return (continuous target)\ndata_flat[\"NextReturn\"] = data_flat.groupby(\"Ticker\")[\"Return\"].shift(-1)\n\n# NextDirection: next-day return direction (binary classification target)\ndata_flat[\"NextDirection\"] = np.where(\n    data_flat[\"NextReturn\"].isna(),\n    np.nan,\n    (data_flat[\"NextReturn\"] > 0).astype(float)\n)\n\n# NextVolSpike: next-day volatility spike (binary classification target)\n# NextVolSpike(t) = 1 if RollingVol at t+1 exceeds 80th percentile of\n# historical RollingVol computed using ONLY data up to t-1\n\nnext_day_vol = data_flat.groupby(\"Ticker\")[\"RollingVol\"].shift(-1)\n\n# Expanding threshold uses ONLY information available up to t-1\n# shift(1) ensures we don't include today's RollingVol in threshold\nexpanding_threshold = data_flat.groupby(\"Ticker\")[\"RollingVol\"].transform(\n    lambda s: s.shift(1).expanding(min_periods=126).quantile(0.8)\n)\n\ndata_flat[\"NextVolSpike\"] = np.where(\n    next_day_vol.isna() | expanding_threshold.isna(),\n    np.nan,\n    (next_day_vol > expanding_threshold).astype(float)\n)\n\n# Sanity check: NaN counts per target\nprint(\"Target NaN counts (from shifting/warm-up windows):\")\nprint(f\"  Lagged_Return: {data_flat['Lagged_Return'].isna().sum()} NaNs\")\nprint(f\"  NextReturn:    {data_flat['NextReturn'].isna().sum()} NaNs\")\nprint(f\"  NextDirection: {data_flat['NextDirection'].isna().sum()} NaNs\")\nprint(f\"  NextVolSpike:  {data_flat['NextVolSpike'].isna().sum()} NaNs\")\nprint(f\"\\nExpected NaNs:\")\nprint(f\"  Lagged_Return: 14 (1 per ticker from shift)\")\nprint(f\"  NextReturn & NextDirection: 14 (1 per ticker from shift)\")\nprint(f\"  NextVolSpike: ~1,890 (126-day warm-up + 1 shift per ticker)\")"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4dc7f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path('data').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save datasets\nfrom pathlib import Path\n\nPath(\"data\").mkdir(exist_ok=True)\n\n# Save FULL dataset (includes rows with incomplete features for EDA)\ndata_flat.to_csv(\"data/merged_features_full.csv\", index=False)\n\n# Drop rows after all feature/label engineering to remove terminal rows\ndata_flat_clean = data_flat.dropna().copy()\n\n# Convert targets to int (safe after dropna)\ndata_flat_clean[\"NextDirection\"] = (\n    data_flat_clean[\"NextDirection\"].astype(int)\n)\ndata_flat_clean[\"NextVolSpike\"] = data_flat_clean[\"NextVolSpike\"].astype(int)\n\n# Save CLEAN dataset (model-ready)\ndata_flat_clean.to_csv(\"data/merged_features_clean.csv\", index=False)\n\n# Summary statistics\nprint(f\"Full dataset (with incomplete features): {data_flat.shape[0]} rows\")\nprint(f\"Clean dataset (model-ready): {data_flat_clean.shape[0]} rows\")\nprint(f\"Rows dropped (incomplete features + NaN targets): \"\n      f\"{data_flat.shape[0] - data_flat_clean.shape[0]}\")\nprint(f\"\\nClean dataset target stats:\")\nprint(f\"  NextDirection: \"\n      f\"{data_flat_clean['NextDirection'].value_counts().to_dict()}\")\nprint(f\"  NextVolSpike:  \"\n      f\"{data_flat_clean['NextVolSpike'].value_counts().to_dict()}\")\n\n# Verify Lagged_Return is NOT identical to Return\nprint(f\"\\nLagged_Return validation:\")\nprint(f\"  Correlation(Return, Lagged_Return): \"\n      f\"{data_flat_clean['Return'].corr(data_flat_clean['Lagged_Return']):.6f}\")\nprint(f\"  Are they identical? \"\n      f\"{(data_flat_clean['Return'] == data_flat_clean['Lagged_Return']).all()}\")\n\ndata_flat_clean.head(10)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Outputs\n",
    "\n",
    "Two datasets are saved for different purposes:\n",
    "\n",
    "1. **`merged_features_full.csv`** - Complete dataset including rows with incomplete features\n",
    "   - **Used for**: EDA price history, understanding full market timeline\n",
    "   - **Contains**: All rows from Jan 2015 onwards, including early 2015 with NaN indicators\n",
    "   - **Rows**: ~38,668\n",
    "\n",
    "2. **`merged_features_clean.csv`** - Model-ready dataset with complete features only\n",
    "   - **Use for**: Feature analysis, correlation matrices, modeling, and predictions\n",
    "   - **Contains**: Only rows where all technical indicators have complete lookback windows\n",
    "   - **Rows**: ~36,764\n",
    "   - **Start date**: July 17, 2015, after 50-day MA, 126-day expanding window requirements are satisfied\n",
    "\n",
    "The separation created here ensures that our EDA can show the complete market picture while modeling uses only reliable, fully-calculated features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}